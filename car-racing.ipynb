{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸï¸ Araba YarÄ±ÅŸÄ±\n",
    "\n",
    "---\n",
    "\n",
    "Bu projede, Gymnasium'dan [Car Racing ortamÄ±](https://gymnasium.farama.org/environments/box2d/car_racing/) ile Ã§alÄ±ÅŸacaksÄ±nÄ±z. GÃ¶rev, prosedÃ¼rel olarak oluÅŸturulan bir pistte arabayla sÃ¼rÃ¼ÅŸ yaparak, yolda kalÄ±rken turlarÄ± verimli bir ÅŸekilde tamamlamaktÄ±r.\n",
    "\n",
    "Ä°ki tÃ¼r pekiÅŸtirmeli Ã¶ÄŸrenme ajanÄ± eÄŸiteceksiniz: bir **DQN ajanÄ±** ve bir **PPO ajanÄ±**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš§ Bu Projeyi FarklÄ± KÄ±lan Nedir?\n",
    "\n",
    "**CliffWalking** gibi **ayrÄ±k eylem uzaylarÄ±** kullanan Ã¶nceki ortamlarÄ±n aksine, Car Racing ortamÄ± **sÃ¼rekli eylem uzayÄ±** kullanÄ±r.\n",
    "\n",
    "- Ajanlar ÅŸunlarÄ± kontrol etmeli:\n",
    "  - **Direksiyon** (sol/saÄŸ)\n",
    "  - **HÄ±zlanma** (gaz)\n",
    "  - **Fren yapma**\n",
    "\n",
    "Bu ÅŸu anlama gelir:\n",
    "- âŒ DQN, ince ayarlÄ± sÃ¼rekli kontrolde zorlanÄ±r  \n",
    "- âœ… PPO Ã§ok daha uygun, Ã§Ã¼nkÃ¼ sÃ¼rekli politikalarÄ± doÄŸrudan Ã¶ÄŸrenebilir\n",
    "\n",
    "ğŸ§  Bu proje, **Politika GradyanÄ± yÃ¶ntemleri** olan PPO'nun neden sÃ¼rekli kontrol problemlerinde tercih edildiÄŸini ve DQN'u yanlÄ±ÅŸ ortama zorladÄ±ÄŸÄ±nÄ±zda ne olduÄŸunu anlamanÄ±za yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bu proje iÃ§in ihtiyaÃ§ duyacaÄŸÄ±mÄ±z tÃ¼m paketleri iÃ§e aktararak baÅŸlayalÄ±m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 1 : CarRacing Ã¼zerinde DQN\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, ÅŸimdiye kadar Ã¶ÄŸrendiÄŸiniz her ÅŸeyi uygulayacaksÄ±nÄ±z â€” ancak bu sefer Gymnasium'dan `CarRacing-v3` ortamÄ± Ã¼zerinde.\n",
    "\n",
    "#### ğŸ§  Ä°lginÃ§ olan ne?  \n",
    "â†’ `CarRacing-v3` **sÃ¼rekli** eylem uzayÄ±na sahip.  \n",
    "â†’ DQN sadece **ayrÄ±k** eylemlerle Ã§alÄ±ÅŸÄ±r.  \n",
    "\n",
    "Bu gÃ¶rev DQN'un temel bir sÄ±nÄ±rÄ±nÄ± vurgular.\n",
    "\n",
    "#### ğŸ“ Ä°zlenecek AdÄ±mlar:\n",
    "\n",
    "1. ğŸš— OrtamÄ± YÃ¼kle: OrtamÄ± yÃ¼klemek iÃ§in `gym.make('CarRacing-v3')` kullan.\n",
    "2. ğŸ§© DummyVecEnv ile Sar.\n",
    "3. âš™ï¸ DQN Modelini BaÅŸlat.\n",
    "4. ğŸªµ EpisodeRewardLogger Kullan:\n",
    "    - Her bÃ¶lÃ¼mÃ¼n sonunda Ã¶dÃ¼lleri kaydetmek iÃ§in aÅŸaÄŸÄ±da saÄŸlanan Ã¶zel geri Ã§aÄŸÄ±rma.\n",
    "    - `.learn()` Ã§aÄŸrÄ±lÄ±rken geri Ã§aÄŸÄ±rma olarak geÃ§irin.\n",
    "5. â±ï¸ 10.000 Zaman AdÄ±mÄ± iÃ§in EÄŸit\n",
    "\n",
    "#### âš ï¸ Ne Olur?\n",
    "\n",
    "```python\n",
    "ValueError: DQN policies can only be used with environments that have a discrete action space. Your action space is of type Box.\n",
    "```\n",
    "\n",
    "Bu hatayÄ± gÃ¶rdÃ¼ÄŸÃ¼nÃ¼zde, bir sonraki bÃ¶lÃ¼me geÃ§meye hazÄ±rsÄ±nÄ±z!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom callback to log rewards at the end of each episode.\n",
    "class EpisodeRewardLogger(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(EpisodeRewardLogger, self).__init__(verbose)\n",
    "        # List to store the total rewards for each episode.\n",
    "        self.episode_rewards = []\n",
    "        # Variable to accumulate rewards for the current episode.\n",
    "        self.episode_reward = 0\n",
    "\n",
    "    # This method is called after every step taken in the environment.\n",
    "    def _on_step(self) -> bool:\n",
    "        # Add the reward from the current step to the episode reward accumulator.\n",
    "        self.episode_reward += self.locals['rewards'][0]\n",
    "\n",
    "        # Check if the episode is done (i.e., the environment has reached a terminal state).\n",
    "        if self.locals['dones'][0]:\n",
    "            # If the episode is done, append the accumulated reward to the episode_rewards list.\n",
    "            self.episode_rewards.append(self.episode_reward)\n",
    "            # Reset the episode reward accumulator to zero for the next episode.\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Return True to continue training, False would stop the training.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â“ Bu neden hata veriyor?\n",
    "\n",
    "- â†’ **DQN** **ayrÄ±k eylem kÃ¼mesi** gerektirir â€” *sol*, *saÄŸ*, *yukarÄ±*, *aÅŸaÄŸÄ±* gibi.  \n",
    "- â†’ **CarRacing** **sÃ¼rekli eylem uzayÄ±** kullanÄ±r â€” *direksiyon*, *hÄ±zlanma* ve *frenleme* iÃ§in gerÃ§el deÄŸerlerle.\n",
    "\n",
    "ğŸ“‰ Bu yÃ¼zden **DQN bu ortamda Ã§alÄ±ÅŸmaz** â€” sÃ¼rekli kontrol gÃ¶revleri iÃ§in inÅŸa edilmemiÅŸ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ§  BÃ¶lÃ¼m 2: Araba YarÄ±ÅŸÄ±nda Politika GradyanÄ±\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, daha Ã¶nce yaptÄ±ÄŸÄ±nÄ±z iÅŸlemi tekrarlayacaksÄ±nÄ±z â€” ancak bu sefer **Politika GradyanÄ± yÃ¶ntemi** kullanarak:  \n",
    "Stable Baselines3'ten `MlpPolicy` ile `PPO` modeli.\n",
    "\n",
    "#### ğŸ“ Ä°zlenecek AdÄ±mlar:\n",
    "\n",
    "1. ğŸš— OrtamÄ± YÃ¼kle: OrtamÄ± yÃ¼klemek iÃ§in `gym.make('CarRacing-v3')` kullan.\n",
    "2. ğŸ§© DummyVecEnv ile Sar.\n",
    "3. âš™ï¸ PPO Modelini BaÅŸlat.\n",
    "4. ğŸªµ EpisodeRewardLogger Kullan:\n",
    "    - Her bÃ¶lÃ¼mÃ¼n sonunda Ã¶dÃ¼lleri kaydetmek iÃ§in aÅŸaÄŸÄ±da saÄŸlanan Ã¶zel geri Ã§aÄŸÄ±rma.\n",
    "    - `.learn()` Ã§aÄŸrÄ±lÄ±rken geri Ã§aÄŸÄ±rma olarak geÃ§irin.\n",
    "5. â±ï¸ 10.000 Zaman AdÄ±mÄ± iÃ§in EÄŸit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ EÄŸitim ne kadar sÃ¼rdÃ¼?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Ã–dÃ¼lleri zaman iÃ§inde Ã§izin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“Š Politika GradyanÄ± EÄŸitim GÃ¼nlÃ¼ÄŸÃ¼ Metriklerini Anlama\n",
    "\n",
    "PPO gibi algoritmalarla eÄŸitim yaparken, Ã§eÅŸitli gÃ¼nlÃ¼k metrikleri performansÄ± ve eÄŸitim kararlÄ±lÄ±ÄŸÄ±nÄ± takip etmenize yardÄ±mcÄ± olur.\n",
    "\n",
    "#### â±ï¸ Zamanla Ä°lgili Metrikler\n",
    "\n",
    "- **fps** â†’ Saniye baÅŸÄ±na kare â€” model ne kadar hÄ±zlÄ± eÄŸitiliyor. YÃ¼ksek = daha iyi.  \n",
    "- **iterations** â†’ Tamamlanan parametre gÃ¼ncelleme dÃ¶ngÃ¼sÃ¼ sayÄ±sÄ±.  \n",
    "- **time_elapsed** â†’ EÄŸitimin baÅŸlamasÄ±ndan bu yana geÃ§en toplam sÃ¼re (saniye).  \n",
    "- **total_timesteps** â†’ Ajan tarafÄ±ndan deneyimlenen toplam ortam adÄ±mÄ± sayÄ±sÄ±.\n",
    "\n",
    "#### ğŸ§  EÄŸitim SÃ¼reci Metrikleri\n",
    "\n",
    "- **approx_kl** â†’ Bir gÃ¼ncellemeden sonra politikanÄ±n ne kadar deÄŸiÅŸtiÄŸi.  \n",
    "  - DÃ¼ÅŸÃ¼k = daha kararlÄ± Ã¶ÄŸrenme.  \n",
    "- **clip_fraction** â†’ PPO'nun gÃ¼ncelleme kÄ±rpmasÄ±nÄ±n ne sÄ±klÄ±kta uygulandÄ±ÄŸÄ±.  \n",
    "- **clip_range** â†’ Politika gÃ¼ncellemelerini kÄ±rpmak iÃ§in kullanÄ±lan aralÄ±k (ani deÄŸiÅŸiklikleri Ã¶nler).  \n",
    "- **entropy_loss** â†’ Eylem seÃ§imindeki rastgeleliÄŸi Ã¶lÃ§er.  \n",
    "  - YÃ¼ksek = daha fazla keÅŸif.  \n",
    "- **explained_variance** â†’ DeÄŸer tahminlerinin gerÃ§ek getirilerle ne kadar eÅŸleÅŸtiÄŸi.  \n",
    "  - 1'e yakÄ±n = daha iyi.  \n",
    "- **learning_rate** â†’ AÄŸÄ±rlÄ±k gÃ¼ncellemelerinin boyutunu kontrol eder.  \n",
    "- **loss** â†’ Genel eÄŸitim kaybÄ± â€” iyileÅŸmeyi takip etmeye yardÄ±mcÄ± olur.  \n",
    "- **n_updates** â†’ Toplam model aÄŸÄ±rlÄ±ÄŸÄ± gÃ¼ncellemesi sayÄ±sÄ±.  \n",
    "- **policy_gradient_loss** â†’ PolitikanÄ±n ne kadar iyileÅŸtiÄŸini gÃ¶sterir.  \n",
    "- **value_loss** â†’ DeÄŸer fonksiyonu tahminlerinin ne kadar doÄŸru olduÄŸunu Ã¶lÃ§er.\n",
    "\n",
    "#### ğŸ” Bu Metrikleri NasÄ±l YorumlarÄ±z\n",
    "\n",
    "- **YÃ¼ksek `entropy_loss`** â†’ Ajan daha fazla keÅŸfediyor (Ã§ok rastgele olabilir).  \n",
    "- **DÃ¼ÅŸÃ¼k `explained_variance`** â†’ DeÄŸer tahminleri yanlÄ±ÅŸ.  \n",
    "- **Azalan `loss`** â†’ Zaman iÃ§inde etkili Ã¶ÄŸrenme olduÄŸunu gÃ¶sterir.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  Politika GradyanÄ± Neden CarRacing iÃ§in Ä°deal\n",
    "\n",
    "CarRacing pÃ¼rÃ¼zsÃ¼z ve hassas kontrol gerektirir â€” bu da **Politika GradyanÄ± yÃ¶ntemleri** olan **PPO**'yu mÃ¼kemmel uyum haline getirir.\n",
    "\n",
    "#### 1. ğŸ¯ SÃ¼rekli Eylemler iÃ§in TasarlandÄ±  \n",
    "- PPO **gerÃ§ek deÄŸerli eylemleri** doÄŸrudan iÅŸler.  \n",
    "- CarRacing'de, ajan sadece sol veya saÄŸÄ± seÃ§mek deÄŸil, *ne kadar direksiyon kÄ±rÄ±lacaÄŸÄ±nÄ±*, *ne kadar sert fren yapÄ±lacaÄŸÄ±nÄ±* ve *ne kadar gaz verileceÄŸini* seÃ§mesi gerekir.\n",
    "\n",
    "#### 2. ğŸ§  PolitikayÄ± DoÄŸrudan Ã–ÄŸrenir  \n",
    "- Eylemlerin deÄŸerini tahmin eden DQN'un aksine, PPO *nasÄ±l* davranacaÄŸÄ±nÄ± Ã¶ÄŸrenir.  \n",
    "- Bu, yÃ¼ksek hÄ±zlÄ± sÃ¼rÃ¼ÅŸ ortamlarÄ±nda kritik olan **daha esnek ve uyarlanabilir davranÄ±ÅŸlara** olanak tanÄ±r.\n",
    "\n",
    "#### 3. ğŸª¶ PÃ¼rÃ¼zsÃ¼z ve Hassas Kontrol  \n",
    "- PPO **ince ayarlÄ± kararlarÄ±** destekler â€” sÃ¼rÃ¼ÅŸ gÃ¶revlerinde gereken ince ayarlamalar iÃ§in mÃ¼kemmel.  \n",
    "- ArtÄ±k ani, hep ya da hiÃ§ hareketler yok.\n",
    "\n",
    "---\n",
    "\n",
    "### âŒ DQN Neden Burada Ä°yi Bir Uyum DeÄŸil\n",
    "\n",
    "- **DQN ayrÄ±k eylem uzayÄ± gerektirir** (Ã¶r. *yukarÄ±*, *aÅŸaÄŸÄ±*, *sol*, *saÄŸ*).  \n",
    "- CarRacing **sÃ¼rekli eylem uzayÄ±** kullanÄ±r â€” eylemlerin gerÃ§el sayÄ±lar olduÄŸu.\n",
    "\n",
    "DQN kullanmak iÃ§in, direksiyon/fren/gaz kombinasyonlarÄ±nÄ±n tamamÄ±nÄ± **ayrÄ±klaÅŸtÄ±rmanÄ±z** gerekir:  \n",
    "ğŸ§± Verimsiz ve nadiren etkili olan bir geÃ§ici Ã§Ã¶zÃ¼m.\n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ Ã–zet: Politika GradyanÄ±nÄ±n ParladÄ±ÄŸÄ± Yer BurasÄ±\n",
    "\n",
    "| YÃ¶ntem           | CarRacing'deki GÃ¼cÃ¼                                    |\n",
    "|------------------|-----------------------------------------------------------|\n",
    "| ğŸ§  Politika GradyanÄ± | SÃ¼rekli eylemleri doÄŸrudan Ã¶ÄŸrenir â€” ayrÄ±klaÅŸtÄ±rma gerekmez. |\n",
    "| âŒ DQN             | SÃ¼rekli eylemler iÃ§in inÅŸa edilmemiÅŸ â€” uyum saÄŸlamakta zorlanÄ±r.         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
